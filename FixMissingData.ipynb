{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FixMissingData.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suren777/Ml-AI-in-finance/blob/master/FixMissingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FqtttFlor3ru",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data fixing method using ANN\n",
        "\n",
        "It is not a secret tha machine learning people spend most of the time fixing datasets and proxying the missing data points. Here we will try to construct artificial neural network to perform this cleaning task for us.\n",
        "\n",
        "The dataset that we are going to use is a daily dataset of yield curves obtained from the US treasury website, which is public. I will briefly discuss the function to download and clean the dataset in the following cells. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Om_36Rf6r2gl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "PLqx2yaUwY4W",
        "colab_type": "code",
        "outputId": "0d18f736-2f0e-4c1a-c76f-ed7e313ffaa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model, save_model, load_model \n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras import regularizers\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from six.moves import urllib\n",
        "import xml.etree.cElementTree as et\n",
        "from copy import deepcopy\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "t1-gGPCCwjgd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data processing\n",
        "Before we jump into the development of the network, let's first get some data. \n",
        "\n",
        "From this link: https://www.treasury.gov/resource-center/data-chart-center/interest-rates/; we can download yearly data for interest rates observed daily. To do so we will use **urllib** module. Unfortunatelly the downloaded file is in xml format and we will have to do some parsing in order to get a working dataset. "
      ]
    },
    {
      "metadata": {
        "id": "M8HHxP-WyExQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "link = r\"https://www.treasury.gov/resource-center/data-chart-center/interest-rates/pages/XmlView.aspx?data=yieldyear&year=\"\n",
        "years = [2000 + y for y  in range(2,19)] # list of years for which we are downloading data "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4EoD6GryYQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets quicly define a function to process the xmls."
      ]
    },
    {
      "metadata": {
        "id": "3vH62S_hyHuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_xml(file):\n",
        "    parseXML = et.parse(file)\n",
        "    curve = []\n",
        "    curve_labels = []\n",
        "    first = True\n",
        "    for node in parseXML.getroot():\n",
        "        try:\n",
        "            aux = node.find(\"{http://www.w3.org/2005/Atom}content\").find('{http://schemas.microsoft.com/ado/2007/08/dataservices/metadata}properties')\n",
        "            curve.append([elem.text for elem in aux])\n",
        "            if first:\n",
        "                curve_labels=[elem.tag.split('}')[1] for elem in aux]\n",
        "                first = False\n",
        "        except:\n",
        "            pass\n",
        "    return curve, curve_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dOEhbWjzyl2d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we define the downloader for the data"
      ]
    },
    {
      "metadata": {
        "id": "txBTV7Pxyknj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def maybe_download(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        first = True\n",
        "        total = []\n",
        "        labels = None\n",
        "        for y in years:\n",
        "            aux= urllib.request.urlopen(link+str(y))\n",
        "            if first:\n",
        "                curve, labels = read_xml(aux) \n",
        "                first                = False\n",
        "            else:\n",
        "                curve, _ = read_xml(aux)\n",
        "            total.extend(deepcopy(curve))\n",
        "        pd.DataFrame(total, columns = labels).sort_values(by='Id').to_csv(filename)\n",
        "\n",
        "maybe_download(\"dataset.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGY9MahSyz-7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, at this point we should have a dataset.csv saved on our drive, lets load it into the dataframe and do some cleaning from NaNs. For the experiments, training and testing we will look at the part of data set where the data rows don't have NaNs. In addition, we will create separate data set which has NaNs in it and we will try to fix it using our trained model. We will call it Neuro Patcher."
      ]
    },
    {
      "metadata": {
        "id": "QtT-nN8qyuRE",
        "colab_type": "code",
        "outputId": "e8f5be89-a18b-44a3-8ab5-58be93e0d7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('dataset.csv')\n",
        "val = dataset.drop(columns=dataset.columns[:3])\n",
        "cols = dataset.columns[3:-1]\n",
        "val = val.values/100\n",
        "val[np.isnan(val)] = 0.0\n",
        "zero_rows = list(set(np.where(val==0)[0]))\n",
        "non_zero_rows = [a for a in range(len(val)) if a not in zero_rows]\n",
        "train = val[non_zero_rows]\n",
        "val = val[zero_rows]\n",
        "print(train[1:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0014 0.0015 0.0018 0.0029 0.0059 0.0099 0.0193 0.0265 0.0332 0.0423\n",
            "  0.0447 0.0447]\n",
            " [0.0015 0.0015 0.0018 0.0026 0.0061 0.0103 0.0199 0.0271 0.034  0.0428\n",
            "  0.0452 0.0452]\n",
            " [0.0014 0.0015 0.0018 0.0027 0.0059 0.01   0.0195 0.0266 0.0335 0.0427\n",
            "  0.0453 0.0453]\n",
            " [0.0015 0.0015 0.0018 0.0026 0.0059 0.01   0.0193 0.0265 0.0334 0.0424\n",
            "  0.045  0.045 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AzB-v6U1CPsu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we need to prepare training data set. Let assume following structure:\n",
        "\n",
        "\n",
        "*   $I_n = \\{\\hat{R}_t(T_0),...,\\hat{R}_t(T_N) \\}$\n",
        "*   $O_n = \\{R_t(T_0),...,R_t(T_N) \\}$\n",
        "\n",
        "where $I_n, O_n$ are inputs and outputs respectively. $R_t(T_n)$ is the yield curve value at time $t$ and tenor $T_n$. $\\hat{R}$ means that this component can be damaged (set to 0).\n",
        "\n",
        "\n",
        "For this purpose we will create a quick function, which takes yield curve as input and the number of elements to damage in the input. "
      ]
    },
    {
      "metadata": {
        "id": "Dt5QDhsvDyGa",
        "colab_type": "code",
        "outputId": "10416751-2cf2-4390-cf7e-845be40a987b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "cell_type": "code",
      "source": [
        "def create_batch(input_data, prob=0.3):\n",
        "  N,M = input_data.shape\n",
        "  inputs = np.zeros((N,M))\n",
        "  labels = np.zeros((N,M))\n",
        "  inputs[:] = input_data\n",
        "  labels[:] = input_data\n",
        "  mask = np.random.choice([False, True], (N,M), p=[1-prob, prob])\n",
        "  inputs[mask] = 0\n",
        "  return inputs,labels\n",
        "\n",
        "inputs, labels = create_batch(train[1:2])\n",
        "for i in range(len(inputs[0])):\n",
        "  print(inputs[0][i],'->', labels[0][i])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0014000000000000002 -> 0.0014000000000000002\n",
            "0.0 -> 0.0015\n",
            "0.0018 -> 0.0018\n",
            "0.0029 -> 0.0029\n",
            "0.0 -> 0.0059\n",
            "0.0 -> 0.009899999999999999\n",
            "0.019299999999999998 -> 0.019299999999999998\n",
            "0.0265 -> 0.0265\n",
            "0.0332 -> 0.0332\n",
            "0.0 -> 0.042300000000000004\n",
            "0.0 -> 0.0447\n",
            "0.0447 -> 0.0447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "olK8oeqWLZ94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally it's time to assemble our model. First lets sample some training data"
      ]
    },
    {
      "metadata": {
        "id": "qo83T94vL0Mc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "n,m = train.shape\n",
        "\n",
        "dataset_max_size = 100000\n",
        "for i in range(dataset_max_size//n):\n",
        "  temp_x,temp_y = create_batch(train)\n",
        "  X.extend(temp_x.tolist())\n",
        "  y.extend(temp_y.tolist()) \n",
        "X = np.array(X)\n",
        "y = np.array(y)  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LR1ioqebFxh4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let setup layers"
      ]
    },
    {
      "metadata": {
        "id": "5jObpfy9NL68",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr=0.0001\n",
        "inp_layer = Input(shape=(m,))\n",
        "lnn1 = Dense(m+2, activation='relu')(inp_layer)\n",
        "lnn2 = Dense(m+2, activation='relu')(lnn1)\n",
        "lnn3 = Dense(m+2, activation='relu')(lnn2)\n",
        "output = Dense(m, activation='linear')(lnn3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E8zAWYiqNYuE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the model"
      ]
    },
    {
      "metadata": {
        "id": "drXbhrg-NXHi",
        "colab_type": "code",
        "outputId": "e41ab0eb-5931-4f0b-e8e7-33d52e0fc1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "cell_type": "code",
      "source": [
        "regressor = Model(inp_layer, output)\n",
        "regressor.compile(optimizer=Adam(lr=lr), loss='mean_squared_error')\n",
        "regressor.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 12)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 14)                182       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 14)                210       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 14)                210       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 12)                180       \n",
            "=================================================================\n",
            "Total params: 782\n",
            "Trainable params: 782\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gQQOH1jCNqVP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ]
    },
    {
      "metadata": {
        "id": "l0Vv-2qfNXzZ",
        "colab_type": "code",
        "outputId": "af362600-8fca-45fe-8680-dac5d79f0dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "regressor.fit(x=X_train, y=y_train, shuffle=True, epochs=5000, batch_size=2058, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc9e4bc0278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "Dd0SK58-kcGP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test score:"
      ]
    },
    {
      "metadata": {
        "id": "pE4_QAJckbUq",
        "colab_type": "code",
        "outputId": "ebdacec7-cdc5-4551-c3d1-b7ffaeb9792e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "score = regressor.evaluate(X_test,y_test, batch_size = 1024)\n",
        "print('Test error:', score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32932/32932 [==============================] - 0s 2us/step\n",
            "Test error: 7.509482618667786e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mOwqRImFNvqt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation = regressor.predict(X_test)\n",
        "\n",
        "pos=len(validation)\n",
        "pos = np.random.choice(len(validation))\n",
        "\n",
        "patched=np.array(X_test[pos])\n",
        "patched[patched==0]=validation[pos,patched==0]\n",
        "\n",
        "plt.plot(validation[pos],'y')\n",
        "plt.plot(y_test[pos],'b-.')\n",
        "plt.plot(X_test[pos],'g*')\n",
        "plt.plot(patched,'co')\n",
        "plt.legend([ 'predicted','actual', 'input', 'patched'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}